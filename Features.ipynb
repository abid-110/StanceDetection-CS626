{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROPf6yVbqVzu",
        "outputId": "3673edd5-4336-4757-c223-d8e83f5beff5"
      },
      "source": [
        "!pip install polyglot\n",
        "!pip install PyICU\n",
        "!pip install pycld2\n",
        "!pip install Morfessor\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from polyglot.text import Word\n",
        "from polyglot.text import Text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting polyglot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/98/e24e2489114c5112b083714277204d92d372f5bbe00d5507acf40370edb9/polyglot-16.7.4.tar.gz (126kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 20.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 81kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 10.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 112kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: polyglot\n",
            "  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52558 sha256=bbb1919f8e3a782845cc481780c5c7ce781b2c4833ba0f8f84ff4320852272bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/91/ef/f1369fdc1203b0a9347d4b24f149b83a305f39ab047986d9da\n",
            "Successfully built polyglot\n",
            "Installing collected packages: polyglot\n",
            "Successfully installed polyglot-16.7.4\n",
            "Collecting PyICU\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/46/fa08c8efae2951e67681ec24319f789fc1a74e2096dd74373e34c79319de/PyICU-2.6.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 10.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyICU\n",
            "  Building wheel for PyICU (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyICU: filename=PyICU-2.6-cp36-cp36m-linux_x86_64.whl size=1288440 sha256=b22e9948b459196dd7857d7ddbab2f92c7798943e588d478d237908d2eb7bfe0\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/21/2f/1c91831e8a93537ab21f6b4b935781b681104635fdb0315791\n",
            "Successfully built PyICU\n",
            "Installing collected packages: PyICU\n",
            "Successfully installed PyICU-2.6\n",
            "Collecting pycld2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/d2/8b0def84a53c88d0eb27c67b05269fbd16ad68df8c78849e7b5d65e6aec3/pycld2-0.41.tar.gz (41.4MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4MB 81kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pycld2\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp36-cp36m-linux_x86_64.whl size=9833514 sha256=1f32ab20002cded987f4caed066b10139ca98dc83cf1981361c6b6ca41f2b33f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/8f/e9/08a1a8932a490175bd140206cd86a3dbcfc70498100de11079\n",
            "Successfully built pycld2\n",
            "Installing collected packages: pycld2\n",
            "Successfully installed pycld2-0.41\n",
            "Collecting Morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AgJk6aV5IJ6",
        "outputId": "9e46571d-400a-4bb3-952e-7fea7cddb926"
      },
      "source": [
        "!polyglot download sentiment2.en"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[polyglot_data] Downloading package sentiment2.en to\n",
            "[polyglot_data]     /root/polyglot_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YFQymk_t99y"
      },
      "source": [
        "def external_features(headline, body):\n",
        "\n",
        "  train_headline_sentences = []\n",
        "  train_body_sentences = []\n",
        "  train_headline_sentences.append(headline)\n",
        "  train_body_sentences.append(body)\n",
        "\n",
        "  ext = []\n",
        "  i = 0\n",
        "  for sent1,sent2 in zip(train_headline_sentences,train_body_sentences):\n",
        "    print(i)\n",
        "    i+=1\n",
        "    vec = []\n",
        "    #character ngrams\n",
        "    for n in range(2,17):\n",
        "      n_grams_1 = ngrams(sent1.lower(), n)\n",
        "      n_grams_2 = ngrams(sent2.lower(),n)\n",
        "      vec.append(len(list(set(n_grams_1).intersection(n_grams_2))))\n",
        "\n",
        "    #word ngrams\n",
        "    for n in range(2,7):\n",
        "      n_grams_1 = ngrams(sent1.lower().split(), n)\n",
        "      n_grams_2 = ngrams(sent2.lower().split(),n)\n",
        "      vec.append(len(list(set(n_grams_1).intersection(n_grams_2))))\n",
        "\n",
        "    #Sentence polarity\n",
        "    flag=False\n",
        "    text1 = Text(sent1)\n",
        "    text2 = Text(sent2)\n",
        "    pol1 = 0\n",
        "    pol2 = 0\n",
        "    for word in text1.words:\n",
        "      try:\n",
        "        pol1+=word.polarity\n",
        "      except:\n",
        "        flag=True\n",
        "        vec.append(0)\n",
        "        break\n",
        "    if (flag==True):\n",
        "      ext.append(vec)\n",
        "      continue\n",
        "    flag=False\n",
        "    for word in text2.words:\n",
        "      try:\n",
        "        pol2+=word.polarity\n",
        "      except:\n",
        "        flag=True\n",
        "        vec.append(0)\n",
        "        break\n",
        "    if (flag==True):\n",
        "      ext.append(vec)\n",
        "      continue\n",
        "    \n",
        "    \n",
        "    pol1 = pol1/(len(sent1.split())*1.0)\n",
        "    pol2 = pol2/(len(sent2.split())*1.0)\n",
        "    print(\"pol1 : \", pol1)\n",
        "    print(\"pol2 : \", pol2)\n",
        "    print(pol1-pol2)\n",
        "    vec.append(pol1-pol2)\n",
        "\n",
        "    ext.append(vec)\n",
        "\n",
        "  return np.array(ext)\n",
        "\n"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkS-thVpItH_"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw_e_5b5Jr_1"
      },
      "source": [
        "def statistical_features(h, b):\n",
        "\n",
        "  v1 = TfidfVectorizer(max_features= 2261)\n",
        "  v2 = TfidfVectorizer(max_features= (10000- 2261))\n",
        "  headline = []\n",
        "  body = []\n",
        "  headline.append(h)\n",
        "  body.append(b)\n",
        "  statistical_h = v1.fit_transform(headline)\n",
        "  statistical_b = v1.fit_transform(body)\n",
        "  #print(statistical_h.shape)\n",
        "  a = statistical_h.toarray()\n",
        "  #a.shape[1]\n",
        "  b = np.zeros((1, 2261 - a.shape[1]))  \n",
        "  #b.shape\n",
        "  statistical_h = np.append(a, b).reshape((1, 2261))\n",
        "  print(statistical_h.shape)\n",
        "  print(statistical_b.shape)\n",
        "  c = statistical_b.toarray()\n",
        "  #c.shape[1]\n",
        "  d = np.zeros((1, 10000 - 2261 - c.shape[1]))\n",
        "  #d.shape\n",
        "  statistical_b = np.append(c, d).reshape((1, 10000-2261))\n",
        "  #print(statistical_b.shape)\n",
        "  final_statistical_features = np.concatenate((statistical_h, statistical_b),axis = 1)\n",
        "  print(final_statistical_features.shape)\n",
        "\n",
        "  return final_statistical_features"
      ],
      "execution_count": 244,
      "outputs": []
    }
  ]
}