{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Depencies. Please run this cell to install external packages needed to run the code\n",
    "!pip install polyglot\n",
    "!polyglot download sgns2.bn\n",
    "!pip install PyICU\n",
    "!pip install pycld2\n",
    "!pip install Morfessor\n",
    "!polyglot download sentiment2.bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the master google drive folder mentioned in the readme file. You will need to download all those data files on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the packages needed\n",
    "import pandas as pd\n",
    "import polyglot\n",
    "from polyglot.text import Text, Word, Downloader\n",
    "import numpy as np\n",
    "from nltk import ngrams, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali_dataset = pd.read_csv('./Data/BengaliSentences.csv')\n",
    "headline = bengali_dataset['headline']\n",
    "body = bengali_dataset['content']\n",
    "labels = [int(x) for x in bengali_dataset['label']]\n",
    "\n",
    "# There are 7202 headline-body pairs in the dataset, labels are 1 for authentic news and 0 for fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Embeddings for the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neural_vec = []\n",
    "for h,b in zip(headline,body):\n",
    "  headline_words = h.split()\n",
    "  body_words = b.split()\n",
    "  head_vec = np.zeros((1,256),dtype='float32')\n",
    "  body_vec = np.zeros((1,256),dtype='float32')\n",
    "  for w in headline_words:\n",
    "    word = Word(w,language='bn')\n",
    "    try:\n",
    "      head_vec+=word.vector.reshape((1,256))\n",
    "    except:\n",
    "      continue\n",
    "  head_vec = head_vec / len(headline_words)\n",
    "\n",
    "  for w in body_words:\n",
    "    word = Word(w,language='bn')\n",
    "    try:\n",
    "      body_vec+=word.vector.reshape((1,256))\n",
    "    except:\n",
    "      continue\n",
    "  body_vec = body_vec / len(body_words)\n",
    "\n",
    "  final_vec = np.concatenate((head_vec,body_vec),axis = 1)\n",
    "\n",
    "  neural_vec.append(final_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the neural embeddings - \n",
    "np.save(arr=np.array(neural_vec),file='./Data/bengali_neural.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Statistical Embeddings for the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique headline words\n",
    "headline_corpus_words = []\n",
    "for head in headline:\n",
    "  for word in head.split():\n",
    "    headline_corpus_words.append(word)\n",
    "headline_corpus_words = set(headline_corpus_words)\n",
    "print(len(headline_corpus_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing term frequency for Headline\n",
    "n = 7202\n",
    "no_words = 2075\n",
    "tf_matrix_head = []\n",
    "for i,head in enumerate(headline):\n",
    "  tf_matrix_line = []\n",
    "  for vocab_word in headline_corpus_words:\n",
    "    if(vocab_word not in head.split()):\n",
    "      tf_matrix_line.append(0)\n",
    "    else:\n",
    "      n_count = 0\n",
    "      for word in head.split():\n",
    "        if (word == vocab_word):\n",
    "          n_count+=1\n",
    "      tf_matrix_line.append(n_count*1.0/len(head.split()))\n",
    "  tf_matrix_head.append(tf_matrix_line)\n",
    "\n",
    "tf_matrix_head = np.array(tf_matrix_head)\n",
    "print(tf_matrix_head.shape) # should be (7202, 12292) dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing idf terms for each headline word\n",
    "idf_headline_corpus_words = []\n",
    "for word in headline_corpus_words:\n",
    "  count = 0\n",
    "  for head in headline:\n",
    "    for h in head.split():\n",
    "      if(h==word):\n",
    "        count+=1\n",
    "        break\n",
    "  idf = np.log(7202.0/count)\n",
    "  idf_headline_corpus_words.append(idf)\n",
    "\n",
    "print(idf_headline_corpus_words) # should be 12292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing IDF terms for headline\n",
    "n = 7202\n",
    "no_words = 2075\n",
    "idf_matrix_head = []\n",
    "for i,head in enumerate(headline):\n",
    "  print(i) \n",
    "  idf_matrix_line = []\n",
    "  for m in range(12292):\n",
    "    idf_matrix_line.append(0)\n",
    "  for word in head.split():\n",
    "    k = -1\n",
    "    for j, vocab_word in enumerate(headline_corpus_words):\n",
    "      if(word==vocab_word):\n",
    "        k = j\n",
    "        break\n",
    "    idf_matrix_line[k] = idf_headline_corpus_words[k]\n",
    "  idf_matrix_head.append(idf_matrix_line)\n",
    "\n",
    "idf_matrix_head = np.array(idf_matrix_head)\n",
    "print(idf_matrix_head.shape) # should be (7202, 12292)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplying these matrices to form the tf-idf terms\n",
    "tfidf_matrix_headline = np.multiply(tf_matrix_head,idf_matrix_head)\n",
    "print(tfidf_matrix_headline.shape) # should be (7202,12292)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing a list of unique body words\n",
    "body_corpus_words = []\n",
    "for b in body:\n",
    "  for word in b.split():\n",
    "    body_corpus_words.append(word)\n",
    "body_corpus_words = set(body_corpus_words)\n",
    "print(len(body_corpus_words)) # should be 117023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing term frequency for body\n",
    "n = 7202\n",
    "no_words = 117023\n",
    "tf_matrix = []\n",
    "for i,b in enumerate(body):\n",
    "  tf_matrix_line = []\n",
    "  for vocab_word in body_corpus_words:\n",
    "    if(vocab_word not in b.split()):\n",
    "      tf_matrix_line.append(0)\n",
    "    else:\n",
    "      n_count = 0\n",
    "      for word in b.split():\n",
    "        if (word == vocab_word):\n",
    "          n_count+=1\n",
    "      tf_matrix_line.append(n_count*1.0/len(b.split()))\n",
    "  tf_matrix.append(tf_matrix_line)\n",
    "\n",
    "tf_matrix_body = np.array(tf_matrix_body)\n",
    "print(tf_matrix_body.shape) # should be (7202,117023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing IDF terms for each body word\n",
    "idf_body_corpus_words = []\n",
    "for i,word in enumerate(body_corpus_words):\n",
    "  print(i)\n",
    "  count = 0\n",
    "  for b in body:\n",
    "    for b_word in b.split():\n",
    "      if(b_word==word):\n",
    "        count+=1\n",
    "        break\n",
    "  idf = np.log(7202.0/count)\n",
    "  idf_body_corpus_words.append(idf)\n",
    "\n",
    "print(len(idf_body_corpus_words)) # should be 117023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing idf term for body sentences\n",
    "n = 7202\n",
    "no_words = 117023\n",
    "idf_matrix_body = []\n",
    "for i,b in enumerate(body):\n",
    "  print(i) \n",
    "  idf_matrix_line = []\n",
    "  for m in range(117023):\n",
    "    idf_matrix_line.append(0)\n",
    "  for word in b.split():\n",
    "    k = -1\n",
    "    for j, vocab_word in enumerate(body_corpus_words):\n",
    "      if(word==vocab_word):\n",
    "        k = j\n",
    "        break\n",
    "    idf_matrix_line[k] = idf_body_corpus_words[k]\n",
    "  idf_matrix_body.append(idf_matrix_line)\n",
    "\n",
    "idf_matrix_body = np.array(idf_matrix_body)\n",
    "print(idf_matrix_body.shape) # should be (7202,117023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplying the tf and idf terms for body to form the final tf-idf matrix\n",
    "tfidf_matrix_body = np.multiply(tf_matrix,idf_matrix)\n",
    "print(tfidf_matrix_body.shape) # should be (7202,117023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining to form the final statistical matrix\n",
    "stat_bn = np.concatenate((tfidf_matrix_headline,tfidf_matrix_body),axis=1)\n",
    "print(stat_bn.shape) # should be (7202,129315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final bengali statistical array\n",
    "np.save(file='./Data/bengali_statistical.npy',arr=stat_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating External Features for Bengali Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the complete external features array to include common word ngrams between headline-body, common character n-grams\n",
    "# between headline-body and relative polarity of headline with respect to the body\n",
    "\n",
    "bn_ext = []\n",
    "i = 0\n",
    "for sent1, sent2 in zip(headlines, body):\n",
    "  print(i)\n",
    "  i+=1\n",
    "  vec = []\n",
    "  for n in range(2,17):\n",
    "\n",
    "    n_grams_1 = ngrams(sent1,n)\n",
    "    n_grams_2 = ngrams(sent2,n)\n",
    "    a = len(list(set(n_grams_1).intersection(n_grams_2)))\n",
    "    vec.append(a)\n",
    "\n",
    "  for n in range(2,7):\n",
    "    n_grams_1 = ngrams(sent1.split(),n)\n",
    "    n_grams_2 = ngrams(sent2.split(),n)\n",
    "    a = len(list(set(n_grams_1).intersection(n_grams_2)))\n",
    "    vec.append(a)\n",
    "    #print(a)\n",
    "\n",
    "  flag=False\n",
    "\n",
    "  text1 = Text(sent1)\n",
    "  text2 = Text(sent2)\n",
    "  pol1 = 0\n",
    "  pol2 = 0\n",
    "\n",
    "  for word in text1.words:\n",
    "    try:\n",
    "      pol1+=word.polarity\n",
    "    except:\n",
    "      flag=True\n",
    "      vec.append(0)\n",
    "      break\n",
    "\n",
    "  if (flag==True):\n",
    "    bn_ext.append(vec)\n",
    "    continue\n",
    "  flag=False\n",
    "  for word in text2.words:\n",
    "    try:\n",
    "      pol2+=word.polarity\n",
    "    except:\n",
    "      flag=True\n",
    "      vec.append(0)\n",
    "      break\n",
    "\n",
    "  if (flag==True):\n",
    "    bn_ext.append(vec)\n",
    "    continue\n",
    "  \n",
    "  pol1 = pol1/(len(sent1.split())*1.0)\n",
    "  pol2 = pol2/(len(sent2.split())*1.0)\n",
    "\n",
    "\n",
    "  vec.append(pol1-pol2)\n",
    "  bn_ext.append(vec)\n",
    "\n",
    "bn_ext = np.array(bn_ext)\n",
    "print(bn_ext.shape)# should be (7202,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the external features bengali array\n",
    "np.save(arr=bn_ext,file='./Data/bengali_external.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
